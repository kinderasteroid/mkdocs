# Quick Start

Get your first quantized model in under 5 minutes.

---

## Python API

```python
from qwodel import Quantizer

quantizer = Quantizer(
    backend="gguf",
    model_path="meta-llama/Llama-2-7b-hf",
    output_dir="./quantized"
)

output_path = quantizer.quantize(format="Q4_K_M")
print(f"Done! Quantized model: {output_path}")
```

That's it. You now have a `.gguf` file ready for deployment.

---

## Command Line

```bash
qwodel quantize meta-llama/Llama-2-7b-hf \
    --backend gguf \
    --format Q4_K_M \
    --output ./quantized
```

---

## Which format should I use?

Not sure what `Q4_K_M` means? Start here:

- **GGUF:** `Q4_K_M` is the recommended default â€” best balance of size and quality.
- **AWQ:** Use `int4` â€” it's the only GPU format.
- **CoreML:** Use `float16` for broadest iOS/macOS compatibility.

See [Concepts â†’ Perplexity vs Speed](../concepts/perplexity-vs-speed.md) for a deeper explanation.

---

## Next Steps

| | |
|---|---|
| ğŸ“š | [Understand the concepts](../concepts/quantization-types.md) â€” Why do different formats exist? |
| âš™ï¸ | [Choose your backend](../guides/choosing-a-backend.md) â€” GGUF, AWQ, or CoreML? |
| ğŸš€ | [Run your model](../integrations/index.md) â€” Load into Ollama, llama.cpp, vLLM, or iOS |
| ğŸ“– | [API Reference](../api-reference/quantizer.md) â€” Full `Quantizer` class docs |
